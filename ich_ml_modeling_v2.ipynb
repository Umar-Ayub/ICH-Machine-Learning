{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from boruta import BorutaPy\n",
    "import warnings\n",
    "import os\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from joblib import dump, load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_imbalance(X, y, method='SMOTE'):\n",
    "    if method == 'SMOTE':\n",
    "        resampler = SMOTE(random_state=42)\n",
    "    elif method == 'undersampling':\n",
    "        resampler = RandomUnderSampler(random_state=42)\n",
    "    X_res, y_res = resampler.fit_resample(X, y)\n",
    "    return X_res, y_res\n",
    "\n",
    "def apply_boruta(X, y, max_depth=5, n_estimators='auto', random_state=42):\n",
    "    rf = RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=max_depth, random_state=random_state)\n",
    "    boruta_selector = BorutaPy(rf, n_estimators=n_estimators, random_state=random_state)\n",
    "    boruta_selector.fit(X.values, y.values)\n",
    "    selected_features = X.columns[boruta_selector.support_].tolist()\n",
    "    X_reduced = X.loc[:, selected_features]\n",
    "    return X_reduced\n",
    "\n",
    "\n",
    "def apply_pca(X, n_components=0.95):\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_reduced = pca.fit_transform(X_scaled)\n",
    "    return X_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../ich_data_w_scores_modified.csv')\n",
    "\n",
    "# Drop specified score columns and reset index\n",
    "score_cols = [\"oICH_score\", \"mICH_score\", \"ICH_GS_score\", \"LSICH_score\", \"ICH_FOS_score\", \"Max_ICH_score\"]\n",
    "df = df.drop(columns=score_cols).reset_index(drop=True)\n",
    "\n",
    "# Features and targets\n",
    "X = df.drop(columns=['MORT90', 'MRS90'])\n",
    "y_mort90 = df['MORT90']\n",
    "y_mrs90 = df['MRS90'].apply(lambda x: 0 if x <= 3 else 1)  # Binarizing MRS90\n",
    "\n",
    "# Splitting the dataset into training and temporary test sets\n",
    "X_train, X_test, y_train_mort90, y_test_mort90 = train_test_split(X, y_mort90, test_size=0.2, random_state=42)\n",
    "X_train, X_test, y_train_mrs90, y_test_mrs90 = train_test_split(X, y_mrs90, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE and undersampling after splitting, only to training data\n",
    "X_train_mort90_smote, y_train_mort90_smote = correct_imbalance(X_train, y_train_mort90, method='SMOTE')\n",
    "X_train_mrs90_smote, y_train_mrs90_smote = correct_imbalance(X_train, y_train_mrs90, method='SMOTE')\n",
    "X_train_mort90_undersample, y_train_mort90_undersample = correct_imbalance(X_train, y_train_mort90, method='undersampling')\n",
    "X_train_mrs90_undersample, y_train_mrs90_undersample = correct_imbalance(X_train, y_train_mrs90, method='undersampling')\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "pca = PCA(n_components=0.95, random_state=42)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "# Example of applying Boruta\n",
    "rf = RandomForestClassifier(n_jobs=-1, max_depth=5, random_state=42)\n",
    "boruta = BorutaPy(rf, n_estimators='auto', verbose=0, random_state=42)\n",
    "boruta.fit(X_train_scaled, y_train_mort90)\n",
    "X_train_boruta = boruta.transform(X_train_scaled)\n",
    "X_test_boruta = boruta.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_lr = LogisticRegression(max_iter=1000)\n",
    "# param_grid_lr = {\n",
    "#     'C': [0.01, 0.1, 1],  # Regularization strength\n",
    "#     'solver': ['liblinear', 'lbfgs']  # Optimization algorithm\n",
    "# }\n",
    "\n",
    "# # Grid search using the validation set for scoring\n",
    "# grid_search_lr = GridSearchCV(model_lr, param_grid_lr, cv=5, scoring='roc_auc')\n",
    "# grid_search_lr.fit(X_train, y_train_mort90)\n",
    "\n",
    "# # Print the best hyperparameters\n",
    "# print(\"Best hyperparameters for Logistic Regression:\")\n",
    "# print(grid_search_lr.best_params_)\n",
    "\n",
    "# # Convert the cv_results_ to a DataFrame\n",
    "# results_df = pd.DataFrame(grid_search_lr.cv_results_)\n",
    "\n",
    "# # Display the first few rows of the results\n",
    "# print(\"Grid Search Results:\")\n",
    "# print(results_df[['param_C', 'param_solver', 'mean_test_score', 'std_test_score', 'rank_test_score']])\n",
    "\n",
    "# # Evaluate on the validation set\n",
    "# y_val_pred_lr = grid_search_lr.predict_proba(X_val)[:, 1]\n",
    "# auc_val_lr = roc_auc_score(y_val_mort90, y_val_pred_lr)\n",
    "# print(f'Logistic Regression Validation AUC: {auc_val_lr}')\n",
    "\n",
    "# y_test_pred_lr = grid_search_lr.predict_proba(X_test)[:, 1]\n",
    "# auc_test_lr = roc_auc_score(y_test_mort90, y_test_pred_lr)\n",
    "# print(f'Logistic Regression Test AUC: {auc_test_lr}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    \"Original mort90\": (X_train, y_train_mort90, X_test, y_test_mort90),\n",
    "    \"SMOTE mort90\": (X_train_mort90_smote, y_train_mort90_smote,  X_test, y_test_mort90),\n",
    "    \"Undersampled mort90\": (X_train_mort90_undersample, y_train_mort90_undersample,  X_test, y_test_mort90),\n",
    "    \"Boruta mort90\": (X_train_boruta, y_train_mort90,  X_test_boruta, y_test_mort90),\n",
    "    \"PCA mort90\": (X_train_pca, y_train_mort90,  X_test_pca, y_test_mort90),\n",
    "    \"Original mrs90\": (X_train, y_train_mrs90, X_test, y_test_mrs90),\n",
    "    \"SMOTE mrs90\": (X_train_mrs90_smote, y_train_mrs90_smote, X_test, y_test_mrs90),\n",
    "    \"Undersampled mrs90\": (X_train_mrs90_undersample, y_train_mrs90_undersample,X_test, y_test_mrs90),\n",
    "    \"Boruta mrs90\": (X_train_boruta, y_train_mrs90, X_test_boruta, y_test_mrs90),\n",
    "    \"PCA mrs90\": (X_train_pca, y_train_mrs90, X_test_pca, y_test_mrs90)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Target              Dataset                         Best Params  Test AUC  \\\n",
      "0  mort90      Original mort90       {'C': 0.1, 'solver': 'lbfgs'}  0.889171   \n",
      "1  mort90         SMOTE mort90     {'C': 1, 'solver': 'liblinear'}  0.832977   \n",
      "2  mort90  Undersampled mort90         {'C': 1, 'solver': 'lbfgs'}  0.869178   \n",
      "3  mort90        Boruta mort90      {'C': 0.01, 'solver': 'lbfgs'}  0.865215   \n",
      "4  mort90           PCA mort90      {'C': 0.01, 'solver': 'lbfgs'}  0.886143   \n",
      "5   mrs90       Original mrs90   {'C': 0.1, 'solver': 'liblinear'}  0.888946   \n",
      "6   mrs90          SMOTE mrs90   {'C': 0.1, 'solver': 'liblinear'}  0.879703   \n",
      "7   mrs90   Undersampled mrs90  {'C': 0.01, 'solver': 'liblinear'}  0.880895   \n",
      "8   mrs90         Boruta mrs90  {'C': 0.01, 'solver': 'liblinear'}  0.884366   \n",
      "9   mrs90            PCA mrs90  {'C': 0.01, 'solver': 'liblinear'}  0.871365   \n",
      "\n",
      "   Precision    Recall        F1  \n",
      "0   0.622222  0.491228  0.549020  \n",
      "1   0.460526  0.614035  0.526316  \n",
      "2   0.325926  0.771930  0.458333  \n",
      "3   0.741935  0.403509  0.522727  \n",
      "4   0.700000  0.368421  0.482759  \n",
      "5   0.839744  0.731844  0.782090  \n",
      "6   0.782857  0.765363  0.774011  \n",
      "7   0.752688  0.782123  0.767123  \n",
      "8   0.811688  0.698324  0.750751  \n",
      "9   0.791411  0.720670  0.754386  \n"
     ]
    }
   ],
   "source": [
    "# Grid search setup\n",
    "param_grid_lr = {\n",
    "    'C': [0.01, 0.1, 1],  # Regularization strength\n",
    "    'solver': ['liblinear', 'lbfgs']  # Optimization algorithm\n",
    "}\n",
    "\n",
    "# Initialize a DataFrame to store results\n",
    "results_columns = ['Target', 'Dataset', 'Best Params', 'Test AUC']\n",
    "results_df = pd.DataFrame(columns=results_columns)\n",
    "\n",
    "# Track the best model\n",
    "best_auc = 0\n",
    "best_model_info = {}\n",
    "\n",
    "for name, (X_train, y_train, X_test, y_test) in datasets.items():\n",
    "    model_lr = LogisticRegression(max_iter=1000)\n",
    "    grid_search_lr = GridSearchCV(model_lr, param_grid_lr, cv=5, scoring='roc_auc')\n",
    "    grid_search_lr.fit(X_train, y_train)\n",
    "\n",
    "    # Best parameters\n",
    "    best_params = grid_search_lr.best_params_\n",
    "\n",
    "    # Evaluation on the test set\n",
    "    y_test_pred_lr = grid_search_lr.predict_proba(X_test)[:, 1]\n",
    "    y_pred = (y_test_pred_lr>= 0.5).astype(int)\n",
    "    auc_test_lr = roc_auc_score(y_test, y_test_pred_lr)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    # Save results\n",
    "    new_row = pd.DataFrame({\n",
    "        'Target': ['mrs90' if 'mrs90' in name else 'mort90'],\n",
    "        'Dataset': [name],\n",
    "        'Best Params': [best_params],\n",
    "        'Test AUC': [auc_test_lr],\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1': f1\n",
    "    })\n",
    "    results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "\n",
    "    # Update best model if this one is better\n",
    "    if auc_test_lr > best_auc:\n",
    "        best_auc = auc_test_lr\n",
    "        best_model_info = {\n",
    "            'model': grid_search_lr.best_estimator_,\n",
    "            'dataset': name,\n",
    "            'target': 'mrs90' if 'mrs90' in name else 'mort90',\n",
    "            'AUC': auc_test_lr,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1': f1\n",
    "        }\n",
    "\n",
    "# Print all results\n",
    "print(results_df)\n",
    "\n",
    "# # Save the best model\n",
    "# if best_model_info:\n",
    "#     dump(best_model_info['model'], f\"best_model_{best_model_info['target']}_{best_model_info['dataset']}.joblib\")\n",
    "#     print(f\"Best model saved from dataset: {best_model_info['dataset']} for target {best_model_info['target']} with Test AUC: {best_auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('lr_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Target              Dataset  \\\n",
      "0   mort90      Original mort90   \n",
      "1   mort90         SMOTE mort90   \n",
      "2   mort90  Undersampled mort90   \n",
      "3   mort90        Boruta mort90   \n",
      "4   mort90           PCA mort90   \n",
      "5    mrs90       Original mrs90   \n",
      "6    mrs90          SMOTE mrs90   \n",
      "7    mrs90   Undersampled mrs90   \n",
      "8    mrs90         Boruta mrs90   \n",
      "9    mrs90            PCA mrs90   \n",
      "10  mort90      Original mort90   \n",
      "11  mort90         SMOTE mort90   \n",
      "12  mort90  Undersampled mort90   \n",
      "13  mort90        Boruta mort90   \n",
      "14  mort90           PCA mort90   \n",
      "15   mrs90       Original mrs90   \n",
      "16   mrs90          SMOTE mrs90   \n",
      "17   mrs90   Undersampled mrs90   \n",
      "18   mrs90         Boruta mrs90   \n",
      "19   mrs90            PCA mrs90   \n",
      "\n",
      "                                          Best Params  Test AUC  Precision  \\\n",
      "0                       {'C': 0.1, 'solver': 'lbfgs'}  0.889171   0.622222   \n",
      "1                     {'C': 1, 'solver': 'liblinear'}  0.832977   0.460526   \n",
      "2                         {'C': 1, 'solver': 'lbfgs'}  0.869178   0.325926   \n",
      "3                      {'C': 0.01, 'solver': 'lbfgs'}  0.865215   0.741935   \n",
      "4                      {'C': 0.01, 'solver': 'lbfgs'}  0.886143   0.700000   \n",
      "5                   {'C': 0.1, 'solver': 'liblinear'}  0.888946   0.839744   \n",
      "6                   {'C': 0.1, 'solver': 'liblinear'}  0.879703   0.782857   \n",
      "7                  {'C': 0.01, 'solver': 'liblinear'}  0.880895   0.752688   \n",
      "8                  {'C': 0.01, 'solver': 'liblinear'}  0.884366   0.811688   \n",
      "9                  {'C': 0.01, 'solver': 'liblinear'}  0.871365   0.791411   \n",
      "10  {'learning_rate': 0.01, 'max_depth': 3, 'n_est...  0.882692   0.880000   \n",
      "11  {'learning_rate': 0.1, 'max_depth': 5, 'n_esti...  0.858447   0.509091   \n",
      "12  {'learning_rate': 0.01, 'max_depth': 3, 'n_est...  0.865950   0.335766   \n",
      "13  {'learning_rate': 0.01, 'max_depth': 3, 'n_est...  0.879486   0.916667   \n",
      "14  {'learning_rate': 0.01, 'max_depth': 3, 'n_est...  0.879531   0.638889   \n",
      "15  {'learning_rate': 0.1, 'max_depth': 3, 'n_esti...  0.882887   0.757396   \n",
      "16  {'learning_rate': 0.1, 'max_depth': 10, 'n_est...  0.864155   0.738889   \n",
      "17  {'learning_rate': 0.01, 'max_depth': 3, 'n_est...  0.889377   0.689815   \n",
      "18  {'learning_rate': 0.1, 'max_depth': 3, 'n_esti...  0.881614   0.767857   \n",
      "19  {'learning_rate': 0.01, 'max_depth': 3, 'n_est...  0.852838   0.777778   \n",
      "\n",
      "      Recall        F1    Model  \n",
      "0   0.491228  0.549020      NaN  \n",
      "1   0.614035  0.526316      NaN  \n",
      "2   0.771930  0.458333      NaN  \n",
      "3   0.403509  0.522727      NaN  \n",
      "4   0.368421  0.482759      NaN  \n",
      "5   0.731844  0.782090      NaN  \n",
      "6   0.765363  0.774011      NaN  \n",
      "7   0.782123  0.767123      NaN  \n",
      "8   0.698324  0.750751      NaN  \n",
      "9   0.720670  0.754386      NaN  \n",
      "10  0.385965  0.536585  XGBoost  \n",
      "11  0.491228  0.500000  XGBoost  \n",
      "12  0.807018  0.474227  XGBoost  \n",
      "13  0.385965  0.543210  XGBoost  \n",
      "14  0.403509  0.494624  XGBoost  \n",
      "15  0.715084  0.735632  XGBoost  \n",
      "16  0.743017  0.740947  XGBoost  \n",
      "17  0.832402  0.754430  XGBoost  \n",
      "18  0.720670  0.743516  XGBoost  \n",
      "19  0.703911  0.739003  XGBoost  \n"
     ]
    }
   ],
   "source": [
    "# Parameter grid for XGBoost\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 10],\n",
    "    'learning_rate': [0.01, 0.1, 0.3]  # Adding learning rate to the grid\n",
    "}\n",
    "\n",
    "# Initialize a DataFrame to store results\n",
    "if 'results_df' not in locals():\n",
    "    results_columns = ['Target', 'Dataset', 'Model', 'Best Params', 'Test AUC', 'Precision', 'Recall', 'F1']\n",
    "    results_df = pd.DataFrame(columns=results_columns)\n",
    "\n",
    "# Track the best model for XGBoost\n",
    "best_auc_xgb = 0\n",
    "best_model_info_xgb = {}\n",
    "\n",
    "for name, (X_train, y_train, X_test, y_test) in datasets.items():\n",
    "    grid_search_xgb = GridSearchCV(\n",
    "        XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "        xgb_param_grid,\n",
    "        cv=5,\n",
    "        scoring='roc_auc'\n",
    "    )\n",
    "    grid_search_xgb.fit(X_train, y_train)\n",
    "\n",
    "    # Best parameters\n",
    "    best_params_xgb = grid_search_xgb.best_params_\n",
    "\n",
    "    # Evaluation on the test set\n",
    "    y_test_pred_xgb = grid_search_xgb.predict_proba(X_test)[:, 1]\n",
    "    y_pred_xgb = (y_test_pred_xgb >= 0.5).astype(int)\n",
    "    auc_test_xgb = roc_auc_score(y_test, y_test_pred_xgb)\n",
    "    precision_xgb = precision_score(y_test, y_pred_xgb)\n",
    "    recall_xgb = recall_score(y_test, y_pred_xgb)\n",
    "    f1_xgb = f1_score(y_test, y_pred_xgb)\n",
    "\n",
    "    # Save results\n",
    "    new_row_xgb = pd.DataFrame({\n",
    "        'Target': ['mrs90' if 'mrs90' in name else 'mort90'],\n",
    "        'Dataset': [name],\n",
    "        'Model': ['XGBoost'],\n",
    "        'Best Params': [best_params_xgb],\n",
    "        'Test AUC': [auc_test_xgb],\n",
    "        'Precision': [precision_xgb],\n",
    "        'Recall': [recall_xgb],\n",
    "        'F1': [f1_xgb]\n",
    "    })\n",
    "    results_df = pd.concat([results_df, new_row_xgb], ignore_index=True)\n",
    "\n",
    "    # Update best model if this one is better\n",
    "    if auc_test_xgb > best_auc_xgb:\n",
    "        best_auc_xgb = auc_test_xgb\n",
    "        best_model_info_xgb = {\n",
    "            'model': grid_search_xgb.best_estimator_,\n",
    "            'dataset': name,\n",
    "            'target': 'mrs90' if 'mrs90' in name else 'mort90',\n",
    "            'AUC': auc_test_xgb,\n",
    "            'Precision': precision_xgb,\n",
    "            'Recall': recall_xgb,\n",
    "            'F1': f1_xgb\n",
    "        }\n",
    "\n",
    "# Print all results\n",
    "print(results_df)\n",
    "\n",
    "# # Optionally save the best XGBoost model\n",
    "# if best_model_info_xgb:\n",
    "#     dump(best_model_info_xgb['model'], f\"best_xgboost_model_{best_model_info_xgb['target']}_{best_model_info_xgb['dataset']}.joblib\")\n",
    "#     print(f\"Best XGBoost model saved from dataset: {best_model_info_xgb['dataset']} for target {best_model_info_xgb['target']} with Test AUC: {best_auc_xgb}\")\n",
    "\n",
    "results_df.to_csv('xgb_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Target              Dataset          Model  \\\n",
      "0  mort90      Original mort90  Random Forest   \n",
      "1  mort90         SMOTE mort90  Random Forest   \n",
      "2  mort90  Undersampled mort90  Random Forest   \n",
      "3  mort90        Boruta mort90  Random Forest   \n",
      "4  mort90           PCA mort90  Random Forest   \n",
      "5   mrs90       Original mrs90  Random Forest   \n",
      "6   mrs90          SMOTE mrs90  Random Forest   \n",
      "7   mrs90   Undersampled mrs90  Random Forest   \n",
      "8   mrs90         Boruta mrs90  Random Forest   \n",
      "9   mrs90            PCA mrs90  Random Forest   \n",
      "\n",
      "                              Best Params  Test AUC  Precision    Recall  \\\n",
      "0  {'max_depth': 10, 'n_estimators': 200}  0.906537   0.733333  0.385965   \n",
      "1  {'max_depth': 10, 'n_estimators': 200}  0.888637   0.514286  0.631579   \n",
      "2  {'max_depth': 10, 'n_estimators': 200}  0.868688   0.350000  0.859649   \n",
      "3   {'max_depth': 3, 'n_estimators': 200}  0.881957   0.840000  0.368421   \n",
      "4  {'max_depth': 10, 'n_estimators': 300}  0.875946   0.608696  0.245614   \n",
      "5  {'max_depth': 10, 'n_estimators': 300}  0.883955   0.800000  0.737430   \n",
      "6  {'max_depth': 10, 'n_estimators': 300}  0.875267   0.754011  0.787709   \n",
      "7  {'max_depth': 10, 'n_estimators': 200}  0.880155   0.720812  0.793296   \n",
      "8  {'max_depth': 10, 'n_estimators': 100}  0.873110   0.768786  0.743017   \n",
      "9  {'max_depth': 10, 'n_estimators': 300}  0.859370   0.797468  0.703911   \n",
      "\n",
      "         F1  \n",
      "0  0.505747  \n",
      "1  0.566929  \n",
      "2  0.497462  \n",
      "3  0.512195  \n",
      "4  0.350000  \n",
      "5  0.767442  \n",
      "6  0.770492  \n",
      "7  0.755319  \n",
      "8  0.755682  \n",
      "9  0.747774  \n"
     ]
    }
   ],
   "source": [
    "# Parameter grid for Random Forest\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 10]\n",
    "}\n",
    "\n",
    "# Initialize a DataFrame to store results\n",
    "if 'results_df_rf' not in locals():\n",
    "    results_columns_rf = ['Target', 'Dataset', 'Model', 'Best Params', 'Test AUC', 'Precision', 'Recall', 'F1']\n",
    "    results_df_rf = pd.DataFrame(columns=results_columns_rf)\n",
    "\n",
    "# Track the best model for Random Forest\n",
    "best_auc_rf = 0\n",
    "best_model_info_rf = {}\n",
    "\n",
    "for name, (X_train, y_train, X_test, y_test) in datasets.items():\n",
    "    grid_search_rf = GridSearchCV(\n",
    "        RandomForestClassifier(),\n",
    "        rf_param_grid,\n",
    "        cv=5,\n",
    "        scoring='roc_auc'\n",
    "    )\n",
    "    grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "    # Best parameters\n",
    "    best_params_rf = grid_search_rf.best_params_\n",
    "\n",
    "    # Evaluation on the test set\n",
    "    y_test_pred_rf = grid_search_rf.predict_proba(X_test)[:, 1]\n",
    "    y_pred_rf = (y_test_pred_rf >= 0.5).astype(int)\n",
    "    auc_test_rf = roc_auc_score(y_test, y_test_pred_rf)\n",
    "    precision_rf = precision_score(y_test, y_pred_rf)\n",
    "    recall_rf = recall_score(y_test, y_pred_rf)\n",
    "    f1_rf = f1_score(y_test, y_pred_rf)\n",
    "\n",
    "    # Save results\n",
    "    new_row_rf = pd.DataFrame({\n",
    "        'Target': ['mrs90' if 'mrs90' in name else 'mort90'],\n",
    "        'Dataset': [name],\n",
    "        'Model': ['Random Forest'],\n",
    "        'Best Params': [best_params_rf],\n",
    "        'Test AUC': [auc_test_rf],\n",
    "        'Precision': [precision_rf],\n",
    "        'Recall': [recall_rf],\n",
    "        'F1': [f1_rf]\n",
    "    })\n",
    "    results_df_rf = pd.concat([results_df_rf, new_row_rf], ignore_index=True)\n",
    "\n",
    "    # Update best model if this one is better\n",
    "    if auc_test_rf > best_auc_rf:\n",
    "        best_auc_rf = auc_test_rf\n",
    "        best_model_info_rf = {\n",
    "            'model': grid_search_rf.best_estimator_,\n",
    "            'dataset': name,\n",
    "            'target': 'mrs90' if 'mrs90' in name else 'mort90',\n",
    "            'AUC': auc_test_rf,\n",
    "            'Precision': precision_rf,\n",
    "            'Recall': recall_rf,\n",
    "            'F1': f1_rf\n",
    "        }\n",
    "\n",
    "# Print all results\n",
    "print(results_df_rf)\n",
    "\n",
    "# Optionally save the best Random Forest model\n",
    "# if best_model_info_rf:\n",
    "#     dump(best_model_info_rf['model'], f\"best_random_forest_model_{best_model_info_rf['target']}_{best_model_info_rf['dataset']}.joblib\")\n",
    "#     print(f\"Best Random Forest model saved from dataset: {best_model_info_rf['dataset']} for target {best_model_info_rf['target']} with Test AUC: {best_auc_rf}\")\n",
    "\n",
    "results_df_rf.to_csv('rf_results.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Target              Dataset Model  \\\n",
      "0  mort90      Original mort90   MLP   \n",
      "1  mort90         SMOTE mort90   MLP   \n",
      "2  mort90  Undersampled mort90   MLP   \n",
      "3  mort90        Boruta mort90   MLP   \n",
      "4  mort90           PCA mort90   MLP   \n",
      "5   mrs90       Original mrs90   MLP   \n",
      "6   mrs90          SMOTE mrs90   MLP   \n",
      "7   mrs90   Undersampled mrs90   MLP   \n",
      "8   mrs90         Boruta mrs90   MLP   \n",
      "9   mrs90            PCA mrs90   MLP   \n",
      "\n",
      "                                         Best Params  Test AUC  Precision  \\\n",
      "0  {'activation': 'tanh', 'alpha': 0.0001, 'hidde...  0.841794   0.500000   \n",
      "1  {'activation': 'tanh', 'alpha': 0.001, 'hidden...  0.839478   0.438356   \n",
      "2  {'activation': 'tanh', 'alpha': 0.001, 'hidden...  0.833200   0.298137   \n",
      "3  {'activation': 'relu', 'alpha': 0.001, 'hidden...  0.858714   0.674419   \n",
      "4  {'activation': 'tanh', 'alpha': 0.01, 'hidden_...  0.868287   0.600000   \n",
      "5  {'activation': 'relu', 'alpha': 0.01, 'hidden_...  0.869003   0.781818   \n",
      "6  {'activation': 'tanh', 'alpha': 0.001, 'hidden...  0.841029   0.721591   \n",
      "7  {'activation': 'relu', 'alpha': 0.01, 'hidden_...  0.853249   0.745946   \n",
      "8  {'activation': 'relu', 'alpha': 0.01, 'hidden_...  0.880977   0.790419   \n",
      "9  {'activation': 'tanh', 'alpha': 0.01, 'hidden_...  0.867503   0.745763   \n",
      "\n",
      "     Recall        F1  \n",
      "0  0.596491  0.544000  \n",
      "1  0.561404  0.492308  \n",
      "2  0.842105  0.440367  \n",
      "3  0.508772  0.580000  \n",
      "4  0.473684  0.529412  \n",
      "5  0.720670  0.750000  \n",
      "6  0.709497  0.715493  \n",
      "7  0.770950  0.758242  \n",
      "8  0.737430  0.763006  \n",
      "9  0.737430  0.741573  \n"
     ]
    }
   ],
   "source": [
    "# Parameter grid for MLP\n",
    "mlp_param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'learning_rate': ['constant', 'adaptive']\n",
    "}\n",
    "\n",
    "# Initialize a DataFrame to store results\n",
    "if 'results_df_mlp' not in locals():\n",
    "    results_columns_mlp = ['Target', 'Dataset', 'Model', 'Best Params', 'Test AUC', 'Precision', 'Recall', 'F1']\n",
    "    results_df_mlp = pd.DataFrame(columns=results_columns_mlp)\n",
    "\n",
    "# Track the best model for MLP\n",
    "best_auc_mlp = 0\n",
    "best_model_info_mlp = {}\n",
    "\n",
    "for name, (X_train, y_train, X_test, y_test) in datasets.items():\n",
    "    grid_search_mlp = GridSearchCV(\n",
    "        MLPClassifier(max_iter=1000),\n",
    "        mlp_param_grid,\n",
    "        cv=5,\n",
    "        scoring='roc_auc'\n",
    "    )\n",
    "    grid_search_mlp.fit(X_train, y_train)\n",
    "\n",
    "    # Best parameters\n",
    "    best_params_mlp = grid_search_mlp.best_params_\n",
    "\n",
    "    # Evaluation on the test set\n",
    "    y_test_pred_mlp = grid_search_mlp.predict_proba(X_test)[:, 1]\n",
    "    y_pred_mlp = (y_test_pred_mlp >= 0.5).astype(int)\n",
    "    auc_test_mlp = roc_auc_score(y_test, y_test_pred_mlp)\n",
    "    precision_mlp = precision_score(y_test, y_pred_mlp)\n",
    "    recall_mlp = recall_score(y_test, y_pred_mlp)\n",
    "    f1_mlp = f1_score(y_test, y_pred_mlp)\n",
    "\n",
    "    # Save results\n",
    "    new_row_mlp = pd.DataFrame({\n",
    "        'Target': ['mrs90' if 'mrs90' in name else 'mort90'],\n",
    "        'Dataset': [name],\n",
    "        'Model': ['MLP'],\n",
    "        'Best Params': [best_params_mlp],\n",
    "        'Test AUC': [auc_test_mlp],\n",
    "        'Precision': [precision_mlp],\n",
    "        'Recall': [recall_mlp],\n",
    "        'F1': [f1_mlp]\n",
    "    })\n",
    "    results_df_mlp = pd.concat([results_df_mlp, new_row_mlp], ignore_index=True)\n",
    "\n",
    "    # Update best model if this one is better\n",
    "    if auc_test_mlp > best_auc_mlp:\n",
    "        best_auc_mlp = auc_test_mlp\n",
    "        best_model_info_mlp = {\n",
    "            'model': grid_search_mlp.best_estimator_,\n",
    "            'dataset': name,\n",
    "            'target': 'mrs90' if 'mrs90' in name else 'mort90',\n",
    "            'AUC': auc_test_mlp,\n",
    "            'Precision': precision_mlp,\n",
    "            'Recall': recall_mlp,\n",
    "            'F1': f1_mlp\n",
    "        }\n",
    "\n",
    "# Print all results\n",
    "print(results_df_mlp)\n",
    "\n",
    "# Optionally save the best MLP model\n",
    "# if best_model_info_mlp:\n",
    "#     dump(best_model_info_mlp['model'], f\"best_mlp_model_{best_model_info_mlp['target']}_{best_model_info_mlp['dataset']}.joblib\")\n",
    "#     print(f\"Best MLP model saved from dataset: {best_model_info_mlp['dataset']} for target {best_model_info_mlp['target']} with Test AUC: {best_auc_mlp}\")\n",
    "\n",
    "results_df_mlp.to_csv('mlp_results.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for Logistic Regression on MRS90:\n",
      "{'C': 0.1, 'solver': 'liblinear'}\n",
      "Grid Search Results for MRS90:\n",
      "  param_C param_solver  mean_test_score  std_test_score  rank_test_score\n",
      "0    0.01    liblinear         0.859068        0.015550                3\n",
      "1    0.01        lbfgs         0.857581        0.016641                4\n",
      "2     0.1    liblinear         0.860442        0.015051                1\n",
      "3     0.1        lbfgs         0.860047        0.015224                2\n",
      "4       1    liblinear         0.856395        0.019148                6\n",
      "5       1        lbfgs         0.857054        0.018999                5\n",
      "Logistic Regression Validation AUC for MRS90: 0.9177838113015442\n",
      "Logistic Regression Test AUC for MRS90: 0.8642628205128206\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Grid search for MRS90\n",
    "grid_search_lr_mrs90 = GridSearchCV(model_lr, param_grid_lr, cv=5, scoring='roc_auc')\n",
    "grid_search_lr_mrs90.fit(X_train, y_train_mrs90)\n",
    "\n",
    "# Print the best hyperparameters for MRS90\n",
    "print(\"Best hyperparameters for Logistic Regression on MRS90:\")\n",
    "print(grid_search_lr_mrs90.best_params_)\n",
    "\n",
    "# Convert the MRS90 cv_results_ to a DataFrame\n",
    "results_df_mrs90 = pd.DataFrame(grid_search_lr_mrs90.cv_results_)\n",
    "print(\"Grid Search Results for MRS90:\")\n",
    "print(results_df_mrs90[['param_C', 'param_solver', 'mean_test_score', 'std_test_score', 'rank_test_score']])\n",
    "\n",
    "# Evaluate MRS90 on the validation set\n",
    "y_val_pred_lr_mrs90 = grid_search_lr_mrs90.predict_proba(X_val)[:, 1]\n",
    "auc_val_lr_mrs90 = roc_auc_score(y_val_mrs90, y_val_pred_lr_mrs90)\n",
    "print(f'Logistic Regression Validation AUC for MRS90: {auc_val_lr_mrs90}')\n",
    "\n",
    "# Evaluate MRS90 on the test set\n",
    "y_test_pred_lr_mrs90 = grid_search_lr_mrs90.predict_proba(X_test)[:, 1]\n",
    "auc_test_lr_mrs90 = roc_auc_score(y_test_mrs90, y_test_pred_lr_mrs90)\n",
    "print(f'Logistic Regression Test AUC for MRS90: {auc_test_lr_mrs90}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter grid for XGBoost\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 10],\n",
    "}\n",
    "\n",
    "# Parameter grid for Random Forest\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 10],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for XGBoost on MORT90: {'max_depth': 3, 'n_estimators': 100}\n",
      "XGBoost Validation AUC for MORT90: 0.8858198451794511\n",
      "XGBoost Test AUC for MORT90: 0.8930375180375181\n"
     ]
    }
   ],
   "source": [
    "# XGBoost Grid Search - MORT90\n",
    "grid_search_xgb_mort90 = GridSearchCV(\n",
    "    XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "    xgb_param_grid,\n",
    "    cv=5,\n",
    "    scoring='roc_auc'\n",
    ")\n",
    "\n",
    "grid_search_xgb_mort90.fit(X_train, y_train_mort90)\n",
    "print(\"Best hyperparameters for XGBoost on MORT90:\", grid_search_xgb_mort90.best_params_)\n",
    "\n",
    "y_val_pred_xgb_mort90 = grid_search_xgb_mort90.predict_proba(X_val)[:, 1]\n",
    "auc_val_xgb_mort90 = roc_auc_score(y_val_mort90, y_val_pred_xgb_mort90)\n",
    "print(f'XGBoost Validation AUC for MORT90: {auc_val_xgb_mort90}')\n",
    "\n",
    "y_test_pred_xgb_mort90 = grid_search_xgb_mort90.predict_proba(X_test)[:, 1]\n",
    "auc_test_xgb_mort90 = roc_auc_score(y_test_mort90, y_test_pred_xgb_mort90)\n",
    "print(f'XGBoost Test AUC for MORT90: {auc_test_xgb_mort90}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for XGBoost on MRS90: {'max_depth': 10, 'n_estimators': 100}\n",
      "XGBoost Validation AUC for MRS90: 0.9046326149669099\n",
      "XGBoost Test AUC for MRS90: 0.8352564102564103\n"
     ]
    }
   ],
   "source": [
    "# XGBoost Grid Search - MRS90\n",
    "grid_search_xgb_mrs90 = GridSearchCV(\n",
    "    XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "    xgb_param_grid,\n",
    "    cv=5,\n",
    "    scoring='roc_auc'\n",
    ")\n",
    "\n",
    "grid_search_xgb_mrs90.fit(X_train, y_train_mrs90)\n",
    "print(\"Best hyperparameters for XGBoost on MRS90:\", grid_search_xgb_mrs90.best_params_)\n",
    "\n",
    "y_val_pred_xgb_mrs90 = grid_search_xgb_mrs90.predict_proba(X_val)[:, 1]\n",
    "auc_val_xgb_mrs90 = roc_auc_score(y_val_mrs90, y_val_pred_xgb_mrs90)\n",
    "print(f'XGBoost Validation AUC for MRS90: {auc_val_xgb_mrs90}')\n",
    "\n",
    "y_test_pred_xgb_mrs90 = grid_search_xgb_mrs90.predict_proba(X_test)[:, 1]\n",
    "auc_test_xgb_mrs90 = roc_auc_score(y_test_mrs90, y_test_pred_xgb_mrs90)\n",
    "print(f'XGBoost Test AUC for MRS90: {auc_test_xgb_mrs90}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for Random Forest on MORT90: {'max_depth': 10, 'n_estimators': 200}\n",
      "Random Forest Validation AUC for MORT90: 0.9210063335679098\n",
      "Random Forest Test AUC for MORT90: 0.8959235209235209\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Grid Search - MORT90\n",
    "grid_search_rf_mort90 = GridSearchCV(\n",
    "    RandomForestClassifier(),\n",
    "    rf_param_grid,\n",
    "    cv=5,\n",
    "    scoring='roc_auc'\n",
    ")\n",
    "grid_search_rf_mort90.fit(X_train, y_train_mort90)\n",
    "print(\"Best hyperparameters for Random Forest on MORT90:\", grid_search_rf_mort90.best_params_)\n",
    "y_val_pred_rf_mort90 = grid_search_rf_mort90.predict_proba(X_val)[:, 1]\n",
    "auc_val_rf_mort90 = roc_auc_score(y_val_mort90, y_val_pred_rf_mort90)\n",
    "print(f'Random Forest Validation AUC for MORT90: {auc_val_rf_mort90}')\n",
    "y_test_pred_rf_mort90 = grid_search_rf_mort90.predict_proba(X_test)[:, 1]\n",
    "auc_test_rf_mort90 = roc_auc_score(y_test_mort90, y_test_pred_rf_mort90)\n",
    "print(f'Random Forest Test AUC for MORT90: {auc_test_rf_mort90}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for Random Forest on MRS90: {'max_depth': 10, 'n_estimators': 300}\n",
      "Random Forest Validation AUC for MRS90: 0.9176989648735788\n",
      "Random Forest Test AUC for MRS90: 0.8519230769230769\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Grid Search - MRS90\n",
    "grid_search_rf_mrs90 = GridSearchCV(\n",
    "    RandomForestClassifier(),\n",
    "    rf_param_grid,\n",
    "    cv=5,\n",
    "    scoring='roc_auc'\n",
    ")\n",
    "grid_search_rf_mrs90.fit(X_train, y_train_mrs90)\n",
    "print(\"Best hyperparameters for Random Forest on MRS90:\", grid_search_rf_mrs90.best_params_)\n",
    "y_val_pred_rf_mrs90 = grid_search_rf_mrs90.predict_proba(X_val)[:, 1]\n",
    "auc_val_rf_mrs90 = roc_auc_score(y_val_mrs90, y_val_pred_rf_mrs90)\n",
    "print(f'Random Forest Validation AUC for MRS90: {auc_val_rf_mrs90}')\n",
    "y_test_pred_rf_mrs90 = grid_search_rf_mrs90.predict_proba(X_test)[:, 1]\n",
    "auc_test_rf_mrs90 = roc_auc_score(y_test_mrs90, y_test_pred_rf_mrs90)\n",
    "print(f'Random Forest Test AUC for MRS90: {auc_test_rf_mrs90}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for MLP on MORT90: {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "MLP Validation AUC for MORT90: 0.8830049261083744\n",
      "MLP Test AUC for MORT90: 0.8336940836940838\n",
      "Best hyperparameters for MLP on MRS90: {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "MLP Validation AUC for MRS90: 0.8910571864924487\n",
      "MLP Test AUC for MRS90: 0.8479967948717948\n"
     ]
    }
   ],
   "source": [
    "# Define the MLP classifier\n",
    "mlp = MLPClassifier(max_iter=1000)\n",
    "\n",
    "# Parameter grid for MLP\n",
    "mlp_param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'learning_rate': ['constant', 'adaptive']\n",
    "}\n",
    "\n",
    "# Grid search for MLP - MORT90\n",
    "grid_search_mlp_mort90 = GridSearchCV(mlp, mlp_param_grid, cv=5, scoring='roc_auc')\n",
    "grid_search_mlp_mort90.fit(X_train, y_train_mort90)\n",
    "print(\"Best hyperparameters for MLP on MORT90:\", grid_search_mlp_mort90.best_params_)\n",
    "y_val_pred_mlp_mort90 = grid_search_mlp_mort90.predict_proba(X_val)[:, 1]\n",
    "auc_val_mlp_mort90 = roc_auc_score(y_val_mort90, y_val_pred_mlp_mort90)\n",
    "print(f'MLP Validation AUC for MORT90: {auc_val_mlp_mort90}')\n",
    "y_test_pred_mlp_mort90 = grid_search_mlp_mort90.predict_proba(X_test)[:, 1]\n",
    "auc_test_mlp_mort90 = roc_auc_score(y_test_mort90, y_test_pred_mlp_mort90)\n",
    "print(f'MLP Test AUC for MORT90: {auc_test_mlp_mort90}')\n",
    "\n",
    "# Grid search for MLP - MRS90\n",
    "grid_search_mlp_mrs90 = GridSearchCV(mlp, mlp_param_grid, cv=5, scoring='roc_auc')\n",
    "grid_search_mlp_mrs90.fit(X_train, y_train_mrs90)\n",
    "print(\"Best hyperparameters for MLP on MRS90:\", grid_search_mlp_mrs90.best_params_)\n",
    "y_val_pred_mlp_mrs90 = grid_search_mlp_mrs90.predict_proba(X_val)[:, 1]\n",
    "auc_val_mlp_mrs90 = roc_auc_score(y_val_mrs90, y_val_pred_mlp_mrs90)\n",
    "print(f'MLP Validation AUC for MRS90: {auc_val_mlp_mrs90}')\n",
    "y_test_pred_mlp_mrs90 = grid_search_mlp_mrs90.predict_proba(X_test)[:, 1]\n",
    "auc_test_mlp_mrs90 = roc_auc_score(y_test_mrs90, y_test_pred_mlp_mrs90)\n",
    "print(f'MLP Test AUC for MRS90: {auc_test_mlp_mrs90}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Test AUC: 0.8834776334776334\n"
     ]
    }
   ],
   "source": [
    "best_lr = grid_search_lr.best_estimator_\n",
    "best_xgb = grid_search_xgb_mort90.best_estimator_\n",
    "best_rf = grid_search_rf_mort90.best_estimator_\n",
    "best_mlp = grid_search_mlp_mort90.best_estimator_\n",
    "\n",
    "\n",
    "# Predict probabilities for the test set\n",
    "lr_probs = best_lr.predict_proba(X_test)[:, 1]\n",
    "xgb_probs = best_xgb.predict_proba(X_test)[:, 1]\n",
    "rf_probs = best_rf.predict_proba(X_test)[:, 1]\n",
    "mlp_probs = best_mlp.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Average the probabilities\n",
    "ensemble_probs = (lr_probs + xgb_probs + rf_probs + mlp_probs) / 4\n",
    "\n",
    "# Evaluate the ensemble\n",
    "ensemble_auc = roc_auc_score(y_test_mort90, ensemble_probs)\n",
    "print(f'Ensemble Test AUC: {ensemble_auc}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stacking classifier\n",
    "Uses a final estimator to blend the outputs of the base estimators. Hereâ€™s an example using a logistic regression as the final estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Ensemble Test AUC: 0.8865440115440116\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "# Define the stacking ensemble\n",
    "stack = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('lr', best_lr),\n",
    "        ('xgb', best_xgb),\n",
    "        ('rf', best_rf),\n",
    "        ('mlp', best_mlp)\n",
    "    ],\n",
    "    final_estimator=LogisticRegression()\n",
    ")\n",
    "\n",
    "# Fit the stack model\n",
    "stack.fit(X_train, y_train_mort90)\n",
    "\n",
    "# Evaluate the stack model on the test set\n",
    "stack_probs = stack.predict_proba(X_test)[:, 1]\n",
    "stack_auc = roc_auc_score(y_test_mort90, stack_probs)\n",
    "print(f'Stacking Ensemble Test AUC: {stack_auc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stacking_model_mort90.joblib']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save Logistic Regression Model\n",
    "dump(grid_search_lr.best_estimator_, 'best_lr_mort90.joblib')\n",
    "\n",
    "# Save XGBoost Model\n",
    "dump(grid_search_xgb_mort90.best_estimator_, 'best_xgb_mort90.joblib')\n",
    "\n",
    "# Save Random Forest Model\n",
    "dump(grid_search_rf_mort90.best_estimator_, 'best_rf_mort90.joblib')\n",
    "\n",
    "# Save MLP Model\n",
    "dump(grid_search_mlp_mort90.best_estimator_, 'best_mlp_mort90.joblib')\n",
    "\n",
    "# If using a stacking classifier\n",
    "dump(stack, 'stacking_model_mort90.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Logistic Regression Model\n",
    "best_lr = load('best_lr_mort90.joblib')\n",
    "\n",
    "# Load XGBoost Model\n",
    "best_xgb = load('best_xgb_mort90.joblib')\n",
    "\n",
    "# Load Random Forest Model\n",
    "best_rf = load('best_rf_mort90.joblib')\n",
    "\n",
    "# Load MLP Model\n",
    "best_mlp = load('best_mlp_mort90.joblib')\n",
    "\n",
    "# Load Stacking Classifier\n",
    "stack = load('stacking_model_mort90.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of using a loaded model to predict\n",
    "predictions = best_lr.predict(X_test)  # Replace X_new with your new data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lr_mrs90 = grid_search_lr_mrs90.best_estimator_\n",
    "best_xgb_mrs90 = grid_search_xgb_mrs90.best_estimator_\n",
    "best_rf_mrs90 = grid_search_rf_mrs90.best_estimator_\n",
    "best_mlp_mrs90 = grid_search_mlp_mrs90.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Validation AUC for MRS90: 0.920159511284575\n",
      "Ensemble Test AUC for MRS90: 0.8610576923076924\n"
     ]
    }
   ],
   "source": [
    "# Predict probabilities for the validation set\n",
    "lr_probs_val = best_lr_mrs90.predict_proba(X_val)[:, 1]\n",
    "xgb_probs_val = best_xgb_mrs90.predict_proba(X_val)[:, 1]\n",
    "rf_probs_val = best_rf_mrs90.predict_proba(X_val)[:, 1]\n",
    "mlp_probs_val = best_mlp_mrs90.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Average the probabilities\n",
    "ensemble_probs_val = (lr_probs_val + xgb_probs_val + rf_probs_val + mlp_probs_val) / 4\n",
    "\n",
    "# Evaluate the ensemble on the validation set\n",
    "ensemble_auc_val = roc_auc_score(y_val_mrs90, ensemble_probs_val)\n",
    "print(f'Ensemble Validation AUC for MRS90: {ensemble_auc_val}')\n",
    "\n",
    "# Predict probabilities for the test set\n",
    "lr_probs_test = best_lr_mrs90.predict_proba(X_test)[:, 1]\n",
    "xgb_probs_test = best_xgb_mrs90.predict_proba(X_test)[:, 1]\n",
    "rf_probs_test = best_rf_mrs90.predict_proba(X_test)[:, 1]\n",
    "mlp_probs_test = best_mlp_mrs90.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Average the probabilities\n",
    "ensemble_probs_test = (lr_probs_test + xgb_probs_test + rf_probs_test + mlp_probs_test) / 4\n",
    "\n",
    "# Evaluate the ensemble on the test set\n",
    "ensemble_auc_test = roc_auc_score(y_test_mrs90, ensemble_probs_test)\n",
    "print(f'Ensemble Test AUC for MRS90: {ensemble_auc_test}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Ensemble Validation AUC for MRS90: 0.9221109791277788\n",
      "Stacking Ensemble Test AUC for MRS90: 0.8592147435897436\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the stacking ensemble\n",
    "stack_mrs90 = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('lr', best_lr_mrs90),\n",
    "        ('xgb', best_xgb_mrs90),\n",
    "        ('rf', best_rf_mrs90),\n",
    "        ('mlp', best_mlp_mrs90)\n",
    "    ],\n",
    "    final_estimator=LogisticRegression()\n",
    ")\n",
    "\n",
    "# Fit the stack model on the training data\n",
    "stack_mrs90.fit(X_train, y_train_mrs90)\n",
    "\n",
    "# Evaluate the stack model on the validation set\n",
    "stack_probs_val = stack_mrs90.predict_proba(X_val)[:, 1]\n",
    "stack_auc_val = roc_auc_score(y_val_mrs90, stack_probs_val)\n",
    "print(f'Stacking Ensemble Validation AUC for MRS90: {stack_auc_val}')\n",
    "\n",
    "# Evaluate the stack model on the test set\n",
    "stack_probs_test = stack_mrs90.predict_proba(X_test)[:, 1]\n",
    "stack_auc_test = roc_auc_score(y_test_mrs90, stack_probs_test)\n",
    "print(f'Stacking Ensemble Test AUC for MRS90: {stack_auc_test}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stacking_model_mrs90.joblib']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save Logistic Regression Model for MRS90\n",
    "dump(best_lr_mrs90, 'best_lr_mrs90.joblib')\n",
    "\n",
    "# Save XGBoost Model for MRS90\n",
    "dump(best_xgb_mrs90, 'best_xgb_mrs90.joblib')\n",
    "\n",
    "# Save Random Forest Model for MRS90\n",
    "dump(best_rf_mrs90, 'best_rf_mrs90.joblib')\n",
    "\n",
    "# Save MLP Model for MRS90\n",
    "dump(best_mlp_mrs90, 'best_mlp_mrs90.joblib')\n",
    "\n",
    "# Save the stacking ensemble for MRS90, if you've created one\n",
    "dump(stack_mrs90, 'stacking_model_mrs90.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Logistic Regression Model for MRS90\n",
    "best_lr_mrs90 = load('best_lr_mrs90.joblib')\n",
    "\n",
    "# Load XGBoost Model for MRS90\n",
    "best_xgb_mrs90 = load('best_xgb_mrs90.joblib')\n",
    "\n",
    "# Load Random Forest Model for MRS90\n",
    "best_rf_mrs90 = load('best_rf_mrs90.joblib')\n",
    "\n",
    "# Load MLP Model for MRS90\n",
    "best_mlp_mrs90 = load('best_mlp_mrs90.joblib')\n",
    "\n",
    "# Load Stacking Model for MRS90\n",
    "stack_mrs90 = load('stacking_model_mrs90.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Use the loaded Random Forest model to predict new data\n",
    "new_data_predictions = best_rf_mrs90.predict(X_test)  # Replace 'new_data' with actual data\n",
    "new_data_predictions2 = stack_mrs90.predict(X_test)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
